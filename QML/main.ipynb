{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d91272f4",
   "metadata": {},
   "source": [
    "# Aprendizaje Automático Cuántico (QML)\n",
    "\n",
    "**Nombre:** Cristian David Araujo A.  \n",
    "**Identificación:** 1089568350  \n",
    "**Fecha:** 11/06/2025  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3212260c",
   "metadata": {},
   "source": [
    "\n",
    "### Descripción del Código\n",
    "\n",
    "El código implementa un clasificador variacional cuántico utilizando la librería PennyLane. A continuación, se describen las etapas principales del flujo de trabajo:\n",
    "\n",
    "1. **Importación de Librerías**: Se importan las librerías necesarias, incluyendo `pennylane`, `numpy`, `sklearn`, y otras para el manejo de datos y métricas de evaluación.\n",
    "\n",
    "2. **Definición de Parámetros y Configuración del Dispositivo**: \n",
    "    - Se define el número de qubits y capas del circuito cuántico.\n",
    "    - Se configura un dispositivo cuántico simulado (`default.qubit`) con los qubits especificados.\n",
    "\n",
    "3. **Feature Map**: Se define la función `statepreparation` para preparar el estado cuántico inicial basado en los datos de entrada.\n",
    "\n",
    "4. **Ansatz**: \n",
    "    - En el primer modelo, se utiliza un Ansatz personalizado con capas definidas manualmente.\n",
    "    - En el segundo modelo, se utiliza el Ansatz `BasicEntanglerLayers`.\n",
    "\n",
    "5. **Circuito Cuántico**: Se define el circuito cuántico utilizando el Ansatz y el Feature Map. El circuito devuelve la expectativa de la puerta Pauli-Z en el primer qubit.\n",
    "\n",
    "6. **Clasificador Variacional**: Se implementa la función `variational_classifier`, que combina el circuito cuántico con un sesgo para realizar predicciones.\n",
    "\n",
    "7. **Funciones de Pérdida y Costo**: \n",
    "    - `square_loss`: Calcula la pérdida cuadrática entre las etiquetas reales y las predicciones.\n",
    "    - `cost`: Evalúa el costo total del modelo basado en las predicciones y las etiquetas.\n",
    "\n",
    "8. **Entrenamiento del Modelo**: \n",
    "    - En el primer modelo, se utiliza el optimizador `AdamOptimizer`.\n",
    "    - En el segundo modelo, se utiliza el optimizador `SPSAOptimizer`.\n",
    "    - Los pesos y sesgos del modelo se actualizan iterativamente para minimizar la función de costo.\n",
    "\n",
    "9. **Evaluación del Modelo**: Se calculan métricas como `accuracy`, `precision`, `recall`, y `F1 Score` para evaluar el desempeño del clasificador cuántico en los datos de prueba.\n",
    "\n",
    "10. **Comparativa de Modelos**: Se realiza una comparación entre dos modelos entrenados con diferentes configuraciones de Ansatz y optimizadores, destacando sus métricas y características.\n",
    "\n",
    "Este flujo de trabajo combina técnicas de aprendizaje automático cuántico con métodos clásicos de preprocesamiento y evaluación de datos para resolver un problema de clasificación binaria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9a59bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:     1 | Cost: 2.3009054 | Accuracy: 0.3657928 \n",
      "Iter:     2 | Cost: 2.0157333 | Accuracy: 0.3657928 \n",
      "Iter:     3 | Cost: 1.6928554 | Accuracy: 0.3657928 \n",
      "Iter:     4 | Cost: 1.4394783 | Accuracy: 0.3657928 \n",
      "Iter:     5 | Cost: 1.2992964 | Accuracy: 0.5205993 \n",
      "Iter:     6 | Cost: 1.2592430 | Accuracy: 0.6167291 \n",
      "Iter:     7 | Cost: 1.2860673 | Accuracy: 0.6167291 \n",
      "Iter:     8 | Cost: 1.3340117 | Accuracy: 0.6167291 \n",
      "Iter:     9 | Cost: 1.3157367 | Accuracy: 0.6167291 \n",
      "Iter:    10 | Cost: 1.2278186 | Accuracy: 0.6167291 \n",
      "Iter:    11 | Cost: 1.1070846 | Accuracy: 0.6167291 \n",
      "Iter:    12 | Cost: 1.0319251 | Accuracy: 0.6167291 \n",
      "Iter:    13 | Cost: 0.9932298 | Accuracy: 0.6167291 \n",
      "Iter:    14 | Cost: 0.9745684 | Accuracy: 0.6167291 \n",
      "Iter:    15 | Cost: 0.9807690 | Accuracy: 0.7802747 \n",
      "Iter:    16 | Cost: 0.9487227 | Accuracy: 0.7802747 \n",
      "Iter:    17 | Cost: 0.9169272 | Accuracy: 0.7802747 \n",
      "Iter:    18 | Cost: 0.8685417 | Accuracy: 0.7802747 \n",
      "Iter:    19 | Cost: 0.8121101 | Accuracy: 0.7802747 \n",
      "Iter:    20 | Cost: 0.7734251 | Accuracy: 0.7802747 \n",
      "Iter:    21 | Cost: 0.7578252 | Accuracy: 0.7802747 \n",
      "Iter:    22 | Cost: 0.7533380 | Accuracy: 0.7802747 \n",
      "Iter:    23 | Cost: 0.7540889 | Accuracy: 0.7727840 \n",
      "Iter:    24 | Cost: 0.7547410 | Accuracy: 0.7727840 \n",
      "Iter:    25 | Cost: 0.7418572 | Accuracy: 0.7727840 \n",
      "Iter:    26 | Cost: 0.7216730 | Accuracy: 0.7727840 \n",
      "Iter:    27 | Cost: 0.7054738 | Accuracy: 0.7840200 \n",
      "Iter:    28 | Cost: 0.6913697 | Accuracy: 0.7840200 \n",
      "Iter:    29 | Cost: 0.6826785 | Accuracy: 0.7840200 \n",
      "Iter:    30 | Cost: 0.6772999 | Accuracy: 0.7840200 \n",
      "Iter:    31 | Cost: 0.6758436 | Accuracy: 0.7840200 \n",
      "Iter:    32 | Cost: 0.6825518 | Accuracy: 0.7840200 \n",
      "Iter:    33 | Cost: 0.6856851 | Accuracy: 0.7840200 \n",
      "Iter:    34 | Cost: 0.6896622 | Accuracy: 0.7840200 \n",
      "Iter:    35 | Cost: 0.6943790 | Accuracy: 0.7840200 \n",
      "Iter:    36 | Cost: 0.7012850 | Accuracy: 0.7840200 \n",
      "Iter:    37 | Cost: 0.7067733 | Accuracy: 0.7840200 \n",
      "Iter:    38 | Cost: 0.7103192 | Accuracy: 0.7840200 \n",
      "Iter:    39 | Cost: 0.7161690 | Accuracy: 0.7840200 \n",
      "Iter:    40 | Cost: 0.7176914 | Accuracy: 0.7840200 \n",
      "Iter:    41 | Cost: 0.7134635 | Accuracy: 0.7840200 \n",
      "Iter:    42 | Cost: 0.7095454 | Accuracy: 0.7840200 \n",
      "Iter:    43 | Cost: 0.7075034 | Accuracy: 0.7840200 \n",
      "Iter:    44 | Cost: 0.7016297 | Accuracy: 0.7840200 \n",
      "Iter:    45 | Cost: 0.6959733 | Accuracy: 0.7840200 \n",
      "Iter:    46 | Cost: 0.6963289 | Accuracy: 0.7840200 \n",
      "Iter:    47 | Cost: 0.6943661 | Accuracy: 0.7840200 \n",
      "Iter:    48 | Cost: 0.6855695 | Accuracy: 0.7840200 \n",
      "Iter:    49 | Cost: 0.6820337 | Accuracy: 0.7840200 \n",
      "Iter:    50 | Cost: 0.6739566 | Accuracy: 0.7840200 \n",
      "Iter:    51 | Cost: 0.6727428 | Accuracy: 0.7840200 \n",
      "Iter:    52 | Cost: 0.6757914 | Accuracy: 0.7840200 \n",
      "Iter:    53 | Cost: 0.6806343 | Accuracy: 0.7840200 \n",
      "Iter:    54 | Cost: 0.6841203 | Accuracy: 0.7840200 \n",
      "Iter:    55 | Cost: 0.6862026 | Accuracy: 0.7840200 \n",
      "Iter:    56 | Cost: 0.6912584 | Accuracy: 0.7840200 \n",
      "Iter:    57 | Cost: 0.6992686 | Accuracy: 0.7852684 \n",
      "Iter:    58 | Cost: 0.7072007 | Accuracy: 0.7852684 \n",
      "Iter:    59 | Cost: 0.7130674 | Accuracy: 0.7852684 \n",
      "Iter:    60 | Cost: 0.7230222 | Accuracy: 0.7852684 \n",
      "Iter:    61 | Cost: 0.7195306 | Accuracy: 0.7852684 \n",
      "Iter:    62 | Cost: 0.7081696 | Accuracy: 0.7852684 \n",
      "Iter:    63 | Cost: 0.7017707 | Accuracy: 0.7852684 \n",
      "Iter:    64 | Cost: 0.6935658 | Accuracy: 0.7840200 \n",
      "Iter:    65 | Cost: 0.6855921 | Accuracy: 0.7840200 \n",
      "Iter:    66 | Cost: 0.6818968 | Accuracy: 0.7840200 \n",
      "Iter:    67 | Cost: 0.6849336 | Accuracy: 0.7840200 \n",
      "Iter:    68 | Cost: 0.6827583 | Accuracy: 0.7840200 \n",
      "Iter:    69 | Cost: 0.6800306 | Accuracy: 0.7840200 \n",
      "Iter:    70 | Cost: 0.6809625 | Accuracy: 0.7840200 \n",
      "Accuracy: 0.7888888888888889\n",
      "Precision: 0.7666666666666667\n",
      "Recall: 0.6571428571428571\n",
      "F1 Score: 0.7712374581939799\n"
     ]
    }
   ],
   "source": [
    "# Importación de librerías necesarias\n",
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "from pennylane.optimize import AdamOptimizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "import math\n",
    "\n",
    "# Definición de parámetros del circuito cuántico\n",
    "num_qubits = 4\n",
    "num_layers = 2\n",
    "\n",
    "# Configuración del dispositivo cuántico\n",
    "dev = qml.device(\"default.qubit\", wires=num_qubits)\n",
    "\n",
    "# Etapa del Feature Map: Preparación del estado cuántico\n",
    "def statepreparation(x):\n",
    "    qml.BasisEmbedding(x, wires=range(0, num_qubits))\n",
    "\n",
    "# Etapa del Ansatz: Definición de las capas del circuito cuántico\n",
    "def layer(W):\n",
    "    qml.Rot(W[0, 0], W[0, 1], W[0, 2], wires=0)\n",
    "    qml.Rot(W[1, 0], W[1, 1], W[1, 2], wires=1)\n",
    "    qml.Rot(W[2, 0], W[2, 1], W[2, 2], wires=2)\n",
    "    qml.Rot(W[3, 0], W[3, 1], W[3, 2], wires=3)\n",
    "\n",
    "    qml.CNOT(wires=[0, 1])\n",
    "    qml.CNOT(wires=[1, 2])\n",
    "    qml.CNOT(wires=[2, 3])\n",
    "    qml.CNOT(wires=[3, 0])\n",
    "\n",
    "# Definición del circuito cuántico\n",
    "@qml.qnode(dev, interface=\"autograd\")\n",
    "def circuit(weights, x):\n",
    "    statepreparation(x)\n",
    "    for W in weights:\n",
    "        layer(W)\n",
    "    return qml.expval(qml.PauliZ(0))\n",
    "\n",
    "# Función del clasificador variacional\n",
    "def variational_classifier(weights, bias, x):\n",
    "    return circuit(weights, x) + bias\n",
    "\n",
    "# Etapa de Función de Pérdida: Definición de la función de pérdida\n",
    "def square_loss(labels, predictions):\n",
    "    loss = 0\n",
    "    for l, p in zip(labels, predictions):\n",
    "        loss = loss + (l - p) ** 2\n",
    "    loss = loss / len(labels)\n",
    "    return loss\n",
    "\n",
    "# Etapa de evaluación de la Función de Costo: Definición de la función de costo\n",
    "def cost(weights, bias, X, Y):\n",
    "    predictions = [variational_classifier(weights, bias, x) for x in X]\n",
    "    return square_loss(Y, predictions)\n",
    "\n",
    "# Etapa de Análisis de Precisión: Definición de la función de precisión\n",
    "def accuracy(labels, predictions):\n",
    "    loss = 0\n",
    "    for l, p in zip(labels, predictions):\n",
    "        if abs(l - p) < 1e-5:\n",
    "            loss = loss + 1\n",
    "    loss = loss / len(labels)\n",
    "    return loss\n",
    "\n",
    "# Preprocesamiento de los datos de entrada\n",
    "df_train = pd.read_csv('train.csv')\n",
    "df_train['Pclass'] = df_train['Pclass'].astype(str)\n",
    "df_train = pd.concat([df_train, pd.get_dummies(df_train[['Pclass', 'Sex', 'Embarked']])], axis=1)\n",
    "df_train['Age'] = df_train['Age'].fillna(df_train['Age'].median())\n",
    "df_train['is_child'] = df_train['Age'].map(lambda x: 1 if x < 12 else 0)\n",
    "cols_model = ['is_child', 'Pclass_1', 'Pclass_2', 'Sex_female']\n",
    "\n",
    "# División de datos de entrenamiento y datos de prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_train[cols_model], df_train['Survived'], test_size=0.10, random_state=42, stratify=df_train['Survived']\n",
    ")\n",
    "\n",
    "# Conversión de datos a formato compatible con PennyLane\n",
    "X_train = np.array(X_train.values, requires_grad=False)\n",
    "Y_train = np.array(y_train.values * 2 - np.ones(len(y_train)), requires_grad=False)\n",
    "\n",
    "# Pesos iniciales definidos\n",
    "np.random.seed(0)\n",
    "weights_init = 0.01 * np.random.randn(num_layers, num_qubits, 3, requires_grad=True)\n",
    "bias_init = np.array(0.0, requires_grad=True)\n",
    "\n",
    "# Configuración del optimizador\n",
    "opt = AdamOptimizer(0.125)\n",
    "num_it = 70\n",
    "batch_size = math.floor(len(X_train) / num_it)\n",
    "\n",
    "# Entrenamiento del clasificador cuántico\n",
    "weights = weights_init\n",
    "bias = bias_init\n",
    "for it in range(num_it):\n",
    "    batch_index = np.random.randint(0, len(X_train), (batch_size,))\n",
    "    X_batch = X_train[batch_index]\n",
    "    Y_batch = Y_train[batch_index]\n",
    "    weights, bias, _, _ = opt.step(cost, weights, bias, X_batch, Y_batch)\n",
    "\n",
    "    # Evaluación de precisión durante el entrenamiento\n",
    "    predictions = [np.sign(variational_classifier(weights, bias, x)) for x in X_train]\n",
    "    acc = accuracy(Y_train, predictions)\n",
    "    print(\n",
    "        \"Iter: {:5d} | Cost: {:0.7f} | Accuracy: {:0.7f} \".format(\n",
    "            it + 1, cost(weights, bias, X_train, Y_train), acc\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Pesos finales entrenados\n",
    "X_test = np.array(X_test.values, requires_grad=False)\n",
    "Y_test = np.array(y_test.values * 2 - np.ones(len(y_test)), requires_grad=False)\n",
    "\n",
    "# Evaluación de desempeño del clasificador cuántico final\n",
    "predictions = [np.sign(variational_classifier(weights, bias, x)) for x in X_test]\n",
    "print(\"Accuracy:\", accuracy_score(Y_test, predictions))\n",
    "print(\"Precision:\", precision_score(Y_test, predictions))\n",
    "print(\"Recall:\", recall_score(Y_test, predictions))\n",
    "print(\"F1 Score:\", f1_score(Y_test, predictions, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe04310",
   "metadata": {},
   "source": [
    "\n",
    "### Actualización del Modelo\n",
    "\n",
    "En esta etapa del flujo de trabajo, se ha realizado un cambio en la configuración del clasificador variacional cuántico:\n",
    "\n",
    "- **Ansatz:** Se utiliza `BasicEntanglerLayers`, que aplica rotaciones parametrizadas y puertas CNOT en una configuración estándar.  \n",
    "- **Optimizador:** Se emplea `SPSAOptimizer`.  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15f346fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:     1 | Cost: 2.3967006 | Accuracy: 0.3657928 \n",
      "Iter:     2 | Cost: 2.2644488 | Accuracy: 0.3657928 \n",
      "Iter:     3 | Cost: 2.2534620 | Accuracy: 0.3657928 \n",
      "Iter:     4 | Cost: 2.0162662 | Accuracy: 0.3657928 \n",
      "Iter:     5 | Cost: 1.9804642 | Accuracy: 0.3657928 \n",
      "Iter:     6 | Cost: 1.9779986 | Accuracy: 0.3657928 \n",
      "Iter:     7 | Cost: 1.8367779 | Accuracy: 0.3657928 \n",
      "Iter:     8 | Cost: 1.8588699 | Accuracy: 0.3657928 \n",
      "Iter:     9 | Cost: 1.8501784 | Accuracy: 0.3657928 \n",
      "Iter:    10 | Cost: 1.7616114 | Accuracy: 0.3657928 \n",
      "Iter:    11 | Cost: 1.6894425 | Accuracy: 0.3657928 \n",
      "Iter:    12 | Cost: 1.5969791 | Accuracy: 0.3657928 \n",
      "Iter:    13 | Cost: 1.5764041 | Accuracy: 0.3657928 \n",
      "Iter:    14 | Cost: 1.5656732 | Accuracy: 0.3657928 \n",
      "Iter:    15 | Cost: 1.5527590 | Accuracy: 0.3657928 \n",
      "Iter:    16 | Cost: 1.5480732 | Accuracy: 0.3657928 \n",
      "Iter:    17 | Cost: 1.4375348 | Accuracy: 0.6167291 \n",
      "Iter:    18 | Cost: 1.4402220 | Accuracy: 0.6167291 \n",
      "Iter:    19 | Cost: 1.4349338 | Accuracy: 0.6167291 \n",
      "Iter:    20 | Cost: 1.4359885 | Accuracy: 0.6167291 \n",
      "Iter:    21 | Cost: 1.4390382 | Accuracy: 0.6167291 \n",
      "Iter:    22 | Cost: 1.4290813 | Accuracy: 0.6167291 \n",
      "Iter:    23 | Cost: 1.3110007 | Accuracy: 0.6167291 \n",
      "Iter:    24 | Cost: 1.2827324 | Accuracy: 0.6167291 \n",
      "Iter:    25 | Cost: 1.2780450 | Accuracy: 0.6167291 \n",
      "Iter:    26 | Cost: 1.2786948 | Accuracy: 0.6167291 \n",
      "Iter:    27 | Cost: 1.2111762 | Accuracy: 0.6167291 \n",
      "Iter:    28 | Cost: 1.2093340 | Accuracy: 0.6167291 \n",
      "Iter:    29 | Cost: 1.1962705 | Accuracy: 0.6167291 \n",
      "Iter:    30 | Cost: 1.1933086 | Accuracy: 0.6167291 \n",
      "Iter:    31 | Cost: 1.1907301 | Accuracy: 0.6167291 \n",
      "Iter:    32 | Cost: 1.1824317 | Accuracy: 0.6167291 \n",
      "Iter:    33 | Cost: 1.1801192 | Accuracy: 0.6167291 \n",
      "Iter:    34 | Cost: 1.1736172 | Accuracy: 0.6167291 \n",
      "Iter:    35 | Cost: 1.1721920 | Accuracy: 0.6167291 \n",
      "Iter:    36 | Cost: 1.1722936 | Accuracy: 0.6167291 \n",
      "Iter:    37 | Cost: 1.1772818 | Accuracy: 0.6167291 \n",
      "Iter:    38 | Cost: 1.1209268 | Accuracy: 0.6167291 \n",
      "Iter:    39 | Cost: 1.1208599 | Accuracy: 0.6167291 \n",
      "Iter:    40 | Cost: 1.1082976 | Accuracy: 0.6167291 \n",
      "Iter:    41 | Cost: 1.1082780 | Accuracy: 0.6167291 \n",
      "Iter:    42 | Cost: 1.1032015 | Accuracy: 0.6167291 \n",
      "Iter:    43 | Cost: 1.0969065 | Accuracy: 0.6167291 \n",
      "Iter:    44 | Cost: 1.0908893 | Accuracy: 0.6167291 \n",
      "Iter:    45 | Cost: 1.0960118 | Accuracy: 0.6167291 \n",
      "Iter:    46 | Cost: 1.0962802 | Accuracy: 0.6167291 \n",
      "Iter:    47 | Cost: 1.0979720 | Accuracy: 0.6167291 \n",
      "Iter:    48 | Cost: 1.0959168 | Accuracy: 0.6167291 \n",
      "Iter:    49 | Cost: 1.0669353 | Accuracy: 0.6167291 \n",
      "Iter:    50 | Cost: 1.0648924 | Accuracy: 0.6167291 \n",
      "Iter:    51 | Cost: 1.0628304 | Accuracy: 0.6167291 \n",
      "Iter:    52 | Cost: 1.0604868 | Accuracy: 0.6167291 \n",
      "Iter:    53 | Cost: 1.0527144 | Accuracy: 0.6167291 \n",
      "Iter:    54 | Cost: 1.0458764 | Accuracy: 0.6167291 \n",
      "Iter:    55 | Cost: 1.0465090 | Accuracy: 0.6167291 \n",
      "Iter:    56 | Cost: 1.0478159 | Accuracy: 0.6167291 \n",
      "Iter:    57 | Cost: 1.0344223 | Accuracy: 0.6167291 \n",
      "Iter:    58 | Cost: 1.0340044 | Accuracy: 0.6167291 \n",
      "Iter:    59 | Cost: 1.0161110 | Accuracy: 0.6167291 \n",
      "Iter:    60 | Cost: 1.0188030 | Accuracy: 0.6167291 \n",
      "Iter:    61 | Cost: 0.9928675 | Accuracy: 0.6167291 \n",
      "Iter:    62 | Cost: 0.9952951 | Accuracy: 0.6167291 \n",
      "Iter:    63 | Cost: 0.9933820 | Accuracy: 0.6167291 \n",
      "Iter:    64 | Cost: 0.9904673 | Accuracy: 0.6167291 \n",
      "Iter:    65 | Cost: 0.9909318 | Accuracy: 0.6167291 \n",
      "Iter:    66 | Cost: 0.9950963 | Accuracy: 0.6167291 \n",
      "Iter:    67 | Cost: 0.9952389 | Accuracy: 0.6167291 \n",
      "Iter:    68 | Cost: 0.9932987 | Accuracy: 0.6167291 \n",
      "Iter:    69 | Cost: 0.9969717 | Accuracy: 0.6167291 \n",
      "Iter:    70 | Cost: 0.9959796 | Accuracy: 0.6167291 \n",
      "Accuracy: 0.6111111111111112\n",
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "F1 Score: 0.5057471264367815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "from pennylane.optimize import SPSAOptimizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "import math\n",
    "\n",
    "# Definición de parámetros del circuito cuántico\n",
    "num_qubits = 4\n",
    "num_layers = 2\n",
    "\n",
    "# Configuración del dispositivo cuántico\n",
    "dev = qml.device(\"default.qubit\", wires=num_qubits)\n",
    "\n",
    "# Etapa del Feature Map: Preparación del estado cuántico\n",
    "def statepreparation(x):\n",
    "    qml.BasisEmbedding(x, wires=range(0, num_qubits))\n",
    "\n",
    "# Etapa del Ansatz: Uso de BasicEntanglerLayers\n",
    "# BasicEntanglerLayers aplica rotaciones de un parámetro en cada qubit seguidas de una cadena cerrada de puertas CNOT.\n",
    "def ansatz(weights):\n",
    "    qml.BasicEntanglerLayers(weights, wires=range(num_qubits))\n",
    "\n",
    "# Definición del circuito cuántico\n",
    "@qml.qnode(dev, interface=\"autograd\")\n",
    "def circuit(weights, x):\n",
    "    statepreparation(x)\n",
    "    ansatz(weights)\n",
    "    return qml.expval(qml.PauliZ(0))\n",
    "\n",
    "# Función del clasificador variacional\n",
    "def variational_classifier(weights, bias, x):\n",
    "    return circuit(weights, x) + bias\n",
    "\n",
    "# Etapa de Función de Pérdida: Definición de la función de pérdida\n",
    "def square_loss(labels, predictions):\n",
    "    loss = 0\n",
    "    for l, p in zip(labels, predictions):\n",
    "        loss = loss + (l - p) ** 2\n",
    "    loss = loss / len(labels)\n",
    "    return loss\n",
    "\n",
    "# Etapa de evaluación de la Función de Costo: Definición de la función de costo\n",
    "def cost(weights, bias, X, Y):\n",
    "    predictions = [variational_classifier(weights, bias, x) for x in X]\n",
    "    return square_loss(Y, predictions)\n",
    "\n",
    "# Etapa de Análisis de Precisión: Definición de la función de precisión\n",
    "def accuracy(labels, predictions):\n",
    "    loss = 0\n",
    "    for l, p in zip(labels, predictions):\n",
    "        if abs(l - p) < 1e-5:\n",
    "            loss = loss + 1\n",
    "    loss = loss / len(labels)\n",
    "    return loss\n",
    "\n",
    "# Preprocesamiento de los datos de entrada\n",
    "df_train = pd.read_csv('train.csv')\n",
    "df_train['Pclass'] = df_train['Pclass'].astype(str)\n",
    "df_train = pd.concat([df_train, pd.get_dummies(df_train[['Pclass', 'Sex', 'Embarked']])], axis=1)\n",
    "df_train['Age'] = df_train['Age'].fillna(df_train['Age'].median())\n",
    "df_train['is_child'] = df_train['Age'].map(lambda x: 1 if x < 12 else 0)\n",
    "cols_model = ['is_child', 'Pclass_1', 'Pclass_2', 'Sex_female']\n",
    "\n",
    "# División de datos de entrenamiento y datos de prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_train[cols_model], df_train['Survived'], test_size=0.10, random_state=42, stratify=df_train['Survived']\n",
    ")\n",
    "\n",
    "# Conversión de datos a formato compatible con PennyLane\n",
    "X_train = np.array(X_train.values, requires_grad=False)\n",
    "Y_train = np.array(y_train.values * 2 - np.ones(len(y_train)), requires_grad=False)\n",
    "\n",
    "# Pesos iniciales definidos\n",
    "np.random.seed(0)\n",
    "weights_init = 0.01 * np.random.randn(num_layers, num_qubits, requires_grad=True)\n",
    "bias_init = np.array(0.0, requires_grad=True)\n",
    "\n",
    "# Configuración del optimizador SPSA\n",
    "opt = SPSAOptimizer(maxiter=70)\n",
    "num_it = 70# Importación de librerías necesarias\n",
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "from pennylane.optimize import SPSAOptimizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "import math\n",
    "\n",
    "# Definición de parámetros del circuito cuántico\n",
    "num_qubits = 4\n",
    "num_layers = 2\n",
    "\n",
    "# Etapa del Feature Map: Preparación del estado cuántico\n",
    "def statepreparation(x):\n",
    "    qml.BasisEmbedding(x, wires=range(0, num_qubits))\n",
    "\n",
    "# Etapa del Ansatz: Uso de BasicEntanglerLayers\n",
    "# BasicEntanglerLayers aplica rotaciones de un parámetro en cada qubit seguidas de una cadena cerrada de puertas CNOT.\n",
    "def ansatz(weights):\n",
    "    qml.BasicEntanglerLayers(weights, wires=range(num_qubits))\n",
    "\n",
    "# Definición del circuito cuántico\n",
    "@qml.qnode(dev, interface=\"autograd\")\n",
    "def circuit(weights, x):\n",
    "    statepreparation(x)\n",
    "    ansatz(weights)\n",
    "    return qml.expval(qml.PauliZ(0))\n",
    "\n",
    "# Función del clasificador variacional\n",
    "def variational_classifier(weights, bias, x):\n",
    "    return circuit(weights, x) + bias\n",
    "\n",
    "# Etapa de Función de Pérdida: Definición de la función de pérdida\n",
    "def square_loss(labels, predictions):\n",
    "    loss = 0\n",
    "    for l, p in zip(labels, predictions):\n",
    "        loss = loss + (l - p) ** 2\n",
    "    loss = loss / len(labels)\n",
    "    return loss\n",
    "\n",
    "# Etapa de evaluación de la Función de Costo: Definición de la función de costo\n",
    "def cost(weights, bias, X, Y):\n",
    "    predictions = [variational_classifier(weights, bias, x) for x in X]\n",
    "    return square_loss(Y, predictions)\n",
    "\n",
    "# Etapa de Análisis de Precisión: Definición de la función de precisión\n",
    "def accuracy(labels, predictions):\n",
    "    loss = 0\n",
    "    for l, p in zip(labels, predictions):\n",
    "        if abs(l - p) < 1e-5:\n",
    "            loss = loss + 1\n",
    "    loss = loss / len(labels)\n",
    "    return loss\n",
    "\n",
    "# Preprocesamiento de los datos de entrada\n",
    "df_train = pd.read_csv('train.csv')\n",
    "df_train['Pclass'] = df_train['Pclass'].astype(str)\n",
    "df_train = pd.concat([df_train, pd.get_dummies(df_train[['Pclass', 'Sex', 'Embarked']])], axis=1)\n",
    "df_train['Age'] = df_train['Age'].fillna(df_train['Age'].median())\n",
    "df_train['is_child'] = df_train['Age'].map(lambda x: 1 if x < 12 else 0)\n",
    "cols_model = ['is_child', 'Pclass_1', 'Pclass_2', 'Sex_female']\n",
    "\n",
    "# División de datos de entrenamiento y datos de prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_train[cols_model], df_train['Survived'], test_size=0.10, random_state=42, stratify=df_train['Survived']\n",
    ")\n",
    "\n",
    "# Conversión de datos a formato compatible con PennyLane\n",
    "X_train = np.array(X_train.values, requires_grad=False)\n",
    "Y_train = np.array(y_train.values * 2 - np.ones(len(y_train)), requires_grad=False)\n",
    "\n",
    "# Pesos iniciales definidos\n",
    "np.random.seed(0)\n",
    "weights_init = 0.01 * np.random.randn(num_layers, num_qubits, requires_grad=True)\n",
    "bias_init = np.array(0.0, requires_grad=True)\n",
    "\n",
    "# Configuración del optimizador SPSA\n",
    "opt = SPSAOptimizer(maxiter=70)\n",
    "num_it = 70\n",
    "batch_size = math.floor(len(X_train) / num_it)\n",
    "\n",
    "# Entrenamiento del clasificador cuántico\n",
    "weights = weights_init\n",
    "bias = bias_init\n",
    "for it in range(num_it):\n",
    "    batch_index = np.random.randint(0, len(X_train), (batch_size,))\n",
    "    X_batch = X_train[batch_index]\n",
    "    Y_batch = Y_train[batch_index]\n",
    "    weights, bias = opt.step(lambda w, b: cost(w, b, X_batch, Y_batch), weights, bias)\n",
    "\n",
    "    # Evaluación de precisión durante el entrenamiento\n",
    "    predictions = [np.sign(variational_classifier(weights, bias, x)) for x in X_train]\n",
    "    acc = accuracy(Y_train, predictions)\n",
    "    print(\n",
    "        \"Iter: {:5d} | Cost: {:0.7f} | Accuracy: {:0.7f} \".format(\n",
    "            it + 1, cost(weights, bias, X_train, Y_train), acc\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Pesos finales entrenados\n",
    "X_test = np.array(X_test.values, requires_grad=False)\n",
    "Y_test = np.array(y_test.values * 2 - np.ones(len(y_test)), requires_grad=False)\n",
    "\n",
    "# Evaluación de desempeño del clasificador cuántico final\n",
    "predictions = [np.sign(variational_classifier(weights, bias, x)) for x in X_test]\n",
    "print(\"Accuracy:\", accuracy_score(Y_test, predictions))\n",
    "print(\"Precision:\", precision_score(Y_test, predictions))\n",
    "print(\"Recall:\", recall_score(Y_test, predictions))\n",
    "print(\"F1 Score:\", f1_score(Y_test, predictions, average='macro'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27e8695",
   "metadata": {},
   "source": [
    "### Comparativa de Modelos\n",
    "\n",
    "#### Modelo 1\n",
    "- **Accuracy:** 0.7889  \n",
    "- **Precision:** 0.7667  \n",
    "- **Recall:** 0.6571  \n",
    "- **F1 Score:** 0.7712  \n",
    "\n",
    "**Características:**  \n",
    "- Utiliza un Ansatz `StronglyEntanglingLayers` personalizado con capas definidas manualmente.  \n",
    "- Optimización realizada con el algoritmo `AdamOptimizer`.  \n",
    "- Presenta un mejor desempeño general en términos de precisión y balance entre las métricas.  \n",
    "- El modelo logra una buena capacidad de predicción, con un equilibrio adecuado entre precisión y sensibilidad.  \n",
    "\n",
    "**Observaciones:**  \n",
    "Este modelo muestra un rendimiento sólido, siendo más confiable para clasificar correctamente los datos. Su precisión y F1 Score indican que es capaz de identificar correctamente las clases con un buen balance entre falsos positivos y falsos negativos.\n",
    "\n",
    "---\n",
    "\n",
    "#### Modelo 2\n",
    "- **Accuracy:** 0.6111  \n",
    "- **Precision:** 0.0  \n",
    "- **Recall:** 0.0  \n",
    "- **F1 Score:** 0.5057  \n",
    "\n",
    "**Características:**  \n",
    "- Utiliza el Ansatz `BasicEntanglerLayers`, que aplica rotaciones parametrizadas y puertas CNOT en una configuración estándar.  \n",
    "- Optimización realizada con el algoritmo `SPSAOptimizer`.  \n",
    "- Presenta un desempeño significativamente inferior en comparación con el Modelo 1.  \n",
    "\n",
    "**Observaciones:**  \n",
    "Este modelo tiene problemas para clasificar correctamente los datos, como lo indican los valores de precisión y recall en 0. Esto podría deberse a una configuración del Ansatz menos adecuada para el problema o a una convergencia insuficiente del optimizador SPSA.\n",
    "\n",
    "---\n",
    "\n",
    "### Conclusión\n",
    "El **Modelo 1** es claramente superior en términos de desempeño, mostrando métricas más altas y un mejor balance entre precisión y sensibilidad. Esto sugiere que el Ansatz personalizado y el uso del optimizador AdamOptimizer son más efectivos para este conjunto de datos y problema específico. Por otro lado, el **Modelo 2** requiere ajustes en su configuración o entrenamiento para mejorar su capacidad predictiva."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
